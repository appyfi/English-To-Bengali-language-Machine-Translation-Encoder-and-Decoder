{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= pd.read_table('ben.txt', names=['eng', 'ben','word'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english=[]\n",
    "bengali=[]\n",
    "for i in lines['eng']:\n",
    "    english.append(i)\n",
    "for j in lines[\"ben\"]:\n",
    "    bengali.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_english=[]\n",
    "for word in english:\n",
    "    word=re.sub('[?|!|.|]',\"\",word)\n",
    "    word=word.lower()\n",
    "    word=re.sub(\"I'm\",\"I am\",word)\n",
    "    word=re.sub(\"can't\",\"can not\",word)\n",
    "    word=re.sub(\"that's\",\"that is\",word)\n",
    "    word=re.sub(\"we're\",\"we are\",word)\n",
    "    word=re.sub(\"I'm\",\"I am\",word)\n",
    "    word=re.sub(\"don't\",\"do not\",word)\n",
    "    word=re.sub(\"we'll\",\"we will\",word)\n",
    "    word=re.sub(\"tom's\",\"tom is\",word)\n",
    "    word=re.sub(\"who'll\",\"who will\",word)\n",
    "    word=re.sub(\"who's\",\"who is\",word)\n",
    "    word=re.sub(\"i'll\",\"i will\",word)\n",
    "    word=re.sub(\"it's\",\"it is\",word)\n",
    "    word=re.sub(\"name's\",\"name is\",word)\n",
    "    word=re.sub(\"she's\",\"she is\",word)\n",
    "    word=re.sub(\"i'\",\"i am\",word)\n",
    "    word=re.sub(\"won't\",\"will not\",word)\n",
    "    clean_english.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bengali=[]\n",
    "for word in bengali:\n",
    "    word=re.sub('[?|!|.|]',\"\",word)\n",
    "    word=re.sub(\"।\",\"\",word)\n",
    "    word=\"START_ \"+word+\" _END\"\n",
    "    clean_bengali.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4617"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_bengali)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START_ যাও _END',\n",
       " 'START_ যান _END',\n",
       " 'START_ যা _END',\n",
       " 'START_ পালাও _END',\n",
       " 'START_ পালান _END',\n",
       " 'START_ কে _END',\n",
       " 'START_ আগুন _END',\n",
       " 'START_ বাঁচাও _END',\n",
       " 'START_ বাঁচান _END',\n",
       " 'START_ থামুন _END']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_bengali[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_bengali=[]\n",
    "length_words=[]\n",
    "for i in clean_bengali:\n",
    "    for word in i.split(\" \"):\n",
    "        length_words.append(len(i.split(\" \")))\n",
    "        if word not in all_words_bengali:\n",
    "            all_words_bengali.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in Bengali 3071\n",
      "Maximum Length is 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique words in Bengali\", len(all_words_bengali))\n",
    "print(\"Maximum Length is\",max(length_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_english=[]\n",
    "length_english=[]\n",
    "for i in clean_english:\n",
    "    for word in i.split(\" \"):\n",
    "        length_english.append(len(i.split(\" \")))\n",
    "        if word not in all_words_english:\n",
    "            all_words_english.append(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in English 1971\n",
      "Maximum Length is 19\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique words in English\", len(all_words_english))\n",
    "print(\"Maximum Length is\",max(length_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_index = dict([(word, i+1) for i, word in enumerate(all_words_english)])\n",
    "bengali_token_index = dict([(word, i+1) for i, word in enumerate(all_words_bengali)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_english_char_index = dict((i, word) for word, i in english_token_index.items())\n",
    "reverse_bengali_char_index = dict((i, word) for word, i in bengali_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(clean_english,clean_bengali,test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "output=[]\n",
    "for i in range(len(X_train)):\n",
    "    seq=[]\n",
    "    for word in X_train[i].split(\" \"):\n",
    "        seq.append(english_token_index[word])\n",
    "    bengali_seq=[]\n",
    "    for words in Y_train[i].split(\" \"):\n",
    "        bengali_seq.append(bengali_token_index[words])\n",
    "    x_pad=pad_sequences([seq],maxlen=19,padding=\"post\")\n",
    "    y_pad=pad_sequences([bengali_seq],maxlen=20,padding=\"post\")\n",
    "    se=np.zeros(3071)\n",
    "    out=bengali_seq[1:]\n",
    "    out_pad=pad_sequences([out],maxlen=20,padding=\"post\")\n",
    "    final_output = to_categorical(out_pad,3072)\n",
    "    \n",
    "    X.append(x_pad)\n",
    "    Y.append(y_pad)\n",
    "    output.append(final_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing The Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_=[]\n",
    "Y_test_=[]\n",
    "output=[]\n",
    "for i in range(len(X_test)):\n",
    "    seq=[]\n",
    "    for word in X_test[i].split(\" \"):\n",
    "        seq.append(english_token_index[word])\n",
    "   \n",
    "    x_pad=pad_sequences([seq],maxlen=19,padding=\"post\")\n",
    "\n",
    "    X_test_.append(x_pad)\n",
    "    Y_test_.append(Y_test[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "for i in X:\n",
    "    x.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in Y:\n",
    "    y.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out=[]\n",
    "for i in output:\n",
    "    all_out.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3924, 19)"
      ]
     },
     "execution_count": 100,
     "metadata": { },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3924, 20)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=np.array(all_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3924, 20, 3072)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_src=19\n",
    "max_length_tar=20\n",
    "latent_dim = 50\n",
    "num_encoder_tokens=1972\n",
    "num_decoder_tokens=3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(num_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 16s 141ms/step - loss: 2.1500 - acc: 0.1528\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 1.6856 - acc: 0.1580\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 6s 148ms/step - loss: 1.6115 - acc: 0.1721\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.5458 - acc: 0.1767\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 1.4936 - acc: 0.2750\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.4463 - acc: 0.3279\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.4036 - acc: 0.3459\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.3663 - acc: 0.3540\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.3339 - acc: 0.3580\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.3062 - acc: 0.3624\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.2816 - acc: 0.3666\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.2596 - acc: 0.3714\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 1.2399 - acc: 0.3800\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.2211 - acc: 0.3854\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.2038 - acc: 0.3891\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.1869 - acc: 0.3918\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.1704 - acc: 0.3938\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.1552 - acc: 0.3968\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.1411 - acc: 0.3999\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 1.1270 - acc: 0.4036\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.1139 - acc: 0.4066\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.1011 - acc: 0.4092\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 1.0882 - acc: 0.4134\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0763 - acc: 0.4168\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0641 - acc: 0.4197\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0528 - acc: 0.4235\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0411 - acc: 0.4279\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0295 - acc: 0.4331\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0177 - acc: 0.4377\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 1.0068 - acc: 0.4435\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9953 - acc: 0.4456\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 0.9841 - acc: 0.4499\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9726 - acc: 0.4539\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.9620 - acc: 0.4583\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9512 - acc: 0.4637\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9400 - acc: 0.4660\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9290 - acc: 0.4717\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.9185 - acc: 0.4769\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.9079 - acc: 0.4803\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 6s 138ms/step - loss: 0.8970 - acc: 0.4836\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.8865 - acc: 0.4872\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.8761 - acc: 0.4928\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.8659 - acc: 0.4973\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.8553 - acc: 0.5003\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.8450 - acc: 0.5052\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.8346 - acc: 0.5101\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.8248 - acc: 0.5146\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.8147 - acc: 0.5175 0s - loss: 0.8147 - acc: 0.517\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.8052 - acc: 0.5221\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7950 - acc: 0.5268\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7850 - acc: 0.5311\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7754 - acc: 0.5348\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7654 - acc: 0.5399\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7560 - acc: 0.5443\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7465 - acc: 0.5488\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.7368 - acc: 0.5526\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.7275 - acc: 0.5584\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.7183 - acc: 0.5615\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.7086 - acc: 0.5666\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.7002 - acc: 0.5720\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.6908 - acc: 0.5770\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.6822 - acc: 0.5814\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.6733 - acc: 0.5863\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.6647 - acc: 0.5887\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.6561 - acc: 0.5939\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.6473 - acc: 0.5974\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 6s 148ms/step - loss: 0.6395 - acc: 0.6032\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.6314 - acc: 0.6056\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 6s 146ms/step - loss: 0.6233 - acc: 0.6094\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.6153 - acc: 0.6138\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.6073 - acc: 0.6168\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5999 - acc: 0.6224\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.5926 - acc: 0.6255\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5849 - acc: 0.6308\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.5773 - acc: 0.6332\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 6s 139ms/step - loss: 0.5702 - acc: 0.6376\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.5635 - acc: 0.6420\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5561 - acc: 0.6450\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5496 - acc: 0.6492\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.5425 - acc: 0.6540\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.5351 - acc: 0.6571\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5291 - acc: 0.6597\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 6s 140ms/step - loss: 0.5224 - acc: 0.6648\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.5160 - acc: 0.6674\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 6s 142ms/step - loss: 0.5094 - acc: 0.6720\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.5032 - acc: 0.6744\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 6s 143ms/step - loss: 0.4971 - acc: 0.6788\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 6s 144ms/step - loss: 0.4907 - acc: 0.6840\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 6s 142ms/step - loss: 0.4847 - acc: 0.6863\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 6s 141ms/step - loss: 0.4787 - acc: 0.6911\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.4731 - acc: 0.6935\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 4s 86ms/step - loss: 0.4669 - acc: 0.6970\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 4s 86ms/step - loss: 0.4614 - acc: 0.7009\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 4s 84ms/step - loss: 0.4552 - acc: 0.7049\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 4s 84ms/step - loss: 0.4500 - acc: 0.7092\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 4s 86ms/step - loss: 0.4447 - acc: 0.7116\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 4s 86ms/step - loss: 0.4384 - acc: 0.7140\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 4s 85ms/step - loss: 0.4339 - acc: 0.7177\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4276 - acc: 0.7209\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4231 - acc: 0.7241\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4184 - acc: 0.7270\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4125 - acc: 0.7319\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4076 - acc: 0.7343\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 3s 83ms/step - loss: 0.4032 - acc: 0.7373\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 4s 84ms/step - loss: 0.3986 - acc: 0.7399\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 4s 83ms/step - loss: 0.3938 - acc: 0.7428: 1s - los\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 4s 83ms/step - loss: 0.3884 - acc: 0.7461\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 4s 84ms/step - loss: 0.3843 - acc: 0.7493\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 4s 85ms/step - loss: 0.3798 - acc: 0.7498: 1s - loss: \n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.3747 - acc: 0.7537\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.3707 - acc: 0.7558\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.3662 - acc: 0.7586\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.3619 - acc: 0.7622\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.3579 - acc: 0.7634\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.3536 - acc: 0.7671\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.3496 - acc: 0.7700\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.3448 - acc: 0.7724\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.3412 - acc: 0.7746\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.3375 - acc: 0.7775\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.3332 - acc: 0.7789\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.3293 - acc: 0.7828\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 4s 91ms/step - loss: 0.3254 - acc: 0.7841\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.3215 - acc: 0.7870\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.3180 - acc: 0.7909\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.3139 - acc: 0.7924: 0s - loss: 0.3130 - acc: \n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.3106 - acc: 0.7940\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.3068 - acc: 0.7964\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.3031 - acc: 0.8008\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.2997 - acc: 0.8022: 2s - los\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 4s 98ms/step - loss: 0.2957 - acc: 0.8049\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.2927 - acc: 0.8069\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.2895 - acc: 0.8099\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.2860 - acc: 0.8139\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 4s 107ms/step - loss: 0.2828 - acc: 0.8143\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 5s 109ms/step - loss: 0.2796 - acc: 0.8175\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.2759 - acc: 0.8201\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.2736 - acc: 0.8227\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 4s 107ms/step - loss: 0.2697 - acc: 0.8244\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.2667 - acc: 0.8257\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.2635 - acc: 0.8282\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.2608 - acc: 0.8281\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.2578 - acc: 0.8306\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 4s 107ms/step - loss: 0.2547 - acc: 0.8333\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.2518 - acc: 0.8367\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.2496 - acc: 0.8364\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.2463 - acc: 0.8398\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2431 - acc: 0.8412\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2407 - acc: 0.8432\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2377 - acc: 0.8455\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.2351 - acc: 0.8483\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.2329 - acc: 0.8483\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.2298 - acc: 0.8501\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.2272 - acc: 0.8540: \n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 4s 95ms/step - loss: 0.2245 - acc: 0.8551\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2224 - acc: 0.8567\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2194 - acc: 0.8585\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2174 - acc: 0.8586\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 4s 94ms/step - loss: 0.2148 - acc: 0.8604\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2122 - acc: 0.8640\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2102 - acc: 0.8652\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.2072 - acc: 0.8663\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 4s 93ms/step - loss: 0.2054 - acc: 0.8668\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 4s 92ms/step - loss: 0.2034 - acc: 0.8690\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 4s 96ms/step - loss: 0.2003 - acc: 0.8706\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 4s 107ms/step - loss: 0.1987 - acc: 0.8727\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 5s 114ms/step - loss: 0.1960 - acc: 0.8745\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 5s 112ms/step - loss: 0.1942 - acc: 0.8760\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 4s 100ms/step - loss: 0.1917 - acc: 0.8779\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.1894 - acc: 0.8785\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.1876 - acc: 0.8793\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.1855 - acc: 0.8820\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.1838 - acc: 0.8828\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.1816 - acc: 0.8844\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 5s 117ms/step - loss: 0.1795 - acc: 0.8852\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 5s 110ms/step - loss: 0.1777 - acc: 0.8858\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 5s 111ms/step - loss: 0.1760 - acc: 0.8869\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 4s 102ms/step - loss: 0.1737 - acc: 0.8883\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 5s 112ms/step - loss: 0.1725 - acc: 0.8897\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 5s 111ms/step - loss: 0.1697 - acc: 0.8906\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 5s 109ms/step - loss: 0.1684 - acc: 0.8931\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.1666 - acc: 0.893 - 5s 110ms/step - loss: 0.1667 - acc: 0.8934\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 5s 108ms/step - loss: 0.1650 - acc: 0.8931\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.1630 - acc: 0.8949\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.1616 - acc: 0.8949\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 4s 106ms/step - loss: 0.1597 - acc: 0.8957\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 5s 120ms/step - loss: 0.1580 - acc: 0.8981\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 6s 137ms/step - loss: 0.1562 - acc: 0.8990\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 5s 113ms/step - loss: 0.1548 - acc: 0.8997\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 5s 129ms/step - loss: 0.1528 - acc: 0.8997\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 5s 116ms/step - loss: 0.1513 - acc: 0.9012\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 5s 108ms/step - loss: 0.1496 - acc: 0.9015\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 4s 105ms/step - loss: 0.1490 - acc: 0.9026\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 5s 108ms/step - loss: 0.1465 - acc: 0.9041\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.1453 - acc: 0.9044\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 4s 99ms/step - loss: 0.1436 - acc: 0.9055\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 4s 103ms/step - loss: 0.1421 - acc: 0.9068\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 4s 104ms/step - loss: 0.1409 - acc: 0.9065\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 4s 101ms/step - loss: 0.1392 - acc: 0.9072\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 4s 100ms/step - loss: 0.1378 - acc: 0.9097\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 4s 97ms/step - loss: 0.1366 - acc: 0.9093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2f1e5f77cc0>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x,y],output,batch_size=95,epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder for Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:: i can not lift my right arm\n",
      "Original-Bengali:: START_ আমি আমার ডান হাতটা তুলতে পারছি না _END\n",
      "Predicted Bengali::   আমার ক্ষমা আর না আমার\n",
      "\n",
      "English:: we are both right\n",
      "Original-Bengali:: START_ আমরা দুজনেই ঠিক _END\n",
      "Predicted Bengali::   আমরা যা করব _END\n",
      "\n",
      "English:: i will call first\n",
      "Original-Bengali:: START_ আমি আগে ফোন করবো _END\n",
      "Predicted Bengali::   আমি তাদের পছন্দ করি\n",
      "\n",
      "English:: my hip hurts\n",
      "Original-Bengali:: START_ আমার কোমর ব্যাথা করছে _END\n",
      "Predicted Bengali::   আমার মালপত্রটা ব্যাথা\n",
      "\n",
      "English:: tom is hurt\n",
      "Original-Bengali:: START_ টম আহত _END\n",
      "Predicted Bengali::   টম আহত _END\n",
      "\n",
      "English:: tom shouted\n",
      "Original-Bengali:: START_ টম চেঁচালো _END\n",
      "Predicted Bengali::   টম চিৎকার করলো _END\n",
      "\n",
      "English:: tom was reading the newspaper when mary walked in\n",
      "Original-Bengali:: START_ টম যখন খবরের কাগজ পড়ছিল তখন মেরি ঘরে ঢুকল _END\n",
      "Predicted Bengali::   টম মেরি কাউকে মেরি\n",
      "\n",
      "English:: i know tom was a friend of yours\n",
      "Original-Bengali:: START_ আমি জানি টম আপনার বন্ধু ছিলেন _END\n",
      "Predicted Bengali::   আমি জানি টম তোমার বন্ধু\n",
      "\n",
      "English:: you should know better\n",
      "Original-Bengali:: START_ তুমিই ভালো করে জানো _END\n",
      "Predicted Bengali::   আপনিই ভালো করে জানেন\n",
      "\n",
      "English:: try again\n",
      "Original-Bengali:: START_ আবার চেষ্টা করুন _END\n",
      "Predicted Bengali::   আবার চেষ্টা করো _END\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    data=X_test_[x]\n",
    "    state_value=encoder_model.predict(data)\n",
    "    target_seq=np.zeros((1,1))\n",
    "    target_seq[0,0]=bengali_token_index[\"START_\"]\n",
    "    \n",
    "    Flag=False\n",
    "    predicted_sentence=\" \"\n",
    "    while not Flag:\n",
    "        output,h,c=decoder_model.predict([target_seq]+state_value)\n",
    "        index=np.argmax(output[0][0])\n",
    "        char=reverse_bengali_char_index[index]\n",
    "        predicted_sentence=predicted_sentence+\" \"+char\n",
    "        if char==\"_END\" or len(predicted_sentence)>19:\n",
    "            Flag=True\n",
    "        target_seq=np.zeros((1,1))\n",
    "        target_seq[0,0]=index\n",
    "        state_value=[h,c]\n",
    "    print(\"English::\", X_test[x])\n",
    "    print(\"Original-Bengali::\", Y_test[x])\n",
    "    print(\"Predicted Bengali::\", predicted_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
